\documentclass{article}
\usepackage{graphicx}
\usepackage{float}

\title{Review Of World Models}
\date{2019-07-08}
\author{Gunjan Arora}

\begin{document}

\maketitle
\pagenumbering{gobble}
\newpage
\pagenumbering{arabic}

\section{Introduction}

In the real world, we make quick decisions and do actions by making future predictions based on what we have experienced before. For instance, if we take baseball example the player on strike is able to predict the position of the ball based on his experience. This is what this reinforcement learning is based on i.e. future predictions. \\
Most of the model-based Reinforcement Learning approaches learn a model of the RL environment but still train on the actual environment. On the other hand, World model fully replaces the actual Reinforcement Learning environment with a generated one and train the agent's controller only inside of the environment which is generated by its own internal model and transfer this policy into the actual environment to perform the action.\\
In the approach of World Models, agent generates environment on its own which may lead to imperfections. To control imperfections, we adjust a temperature parameter of the internal world model to control the amount of uncertainty. Also, the author trained the agent's controller inside of a noisier and more uncertain version of its generated environment. This approach helped to prevent the agent from taking advantage of the imperfections of its internal world.

\section{Agent Model}

Their are three components of the agent:\\
\begin{enumerate}
	\item Visual Sensory Component(V)
	\item Memory Component(M)
	\item Decision making Component(C)
\end{enumerate}

\begin{figure}[H]
  \includegraphics[width=\linewidth]{1.png}
  \caption{Agent shown pictorially}
\end{figure}

\section{VAE(V) Model}
Role of V model is to learn an abstract, compressed representation of each observed input frame. Variational Autoencoder (VAE) was used as the V model in experiments. A frame that is received at time t is converted to low dimensional latent vector which can be used to reconstruct the original image.

\section{MDN-RNN (M) Model}
Role of M model is to predict future. M model takes the input from the V model generated at time t and past predictions and actions of the agent and use all these inputs to predict future z vectors which V is expected to produce after time t. All the input(past actions, history and present latent vector z) are fed into RNN model, the output of which is passed on to MDN whose other input is temperature(to control uncertainity). Temperature parameter can be adjusted.
\begin{figure}[H]
  \includegraphics[width=\linewidth]{2.png}
  \caption{M Model}
\end{figure}

\section{Controller(C) Model}
The C model is responsible for determining the course of actions to take in order to maximize the expected cumulative reward of the agent during a rollout of the environment.\\
C is kept simple and small as possible and trained seperately from V and M model, so that most of the agent's complexity resides in V and M model.\\
C is a simple layer that maps z\textsubscript{t} and h\textsubscript{t} directly to action a\textsubscript{t} at each time step:
\begin{figure}[H]
  \includegraphics[width=\linewidth]{3.png}
\end{figure}

\section{Combining Models}
The following flow diagram illustrates how V, M, and C interacts with the environment:
\begin{figure}[H]
  \includegraphics[width=\linewidth]{4.png}
\end{figure}
Car racing experiment performed by the author is used in this review to explain how C, V and M models are put together\\
To train the V model, a dataset of 10,000 random rollouts of the environment are first collected. First agent acts randomly to explore the environment multiple times and a\textsubscript{t} actions are recorded and also the observations from the environment are recorded.\\
Then VAE is trained to encode each frame into low dimensional vector z by minimizing the difference between the given frame and the reconstructed version of the frame produced by the decoder from z.\\
After this model M is trained using the pre-processed frame z\textsubscript{t}. Using this pre-processed data along with the recorded random actions a\textsubscript{t} taken, MDN-RNN architecture can be trained to model P(z\textsubscript{t+1}$\vert$ a\textsubscript{t}, z\textsubscript{t}, h\textsubscript{t}) as a mixture of Gaussians.\\
After this the controller C is evolved to maximize the expected cumulative reward of a rollout.

\end{document}